[NeuralNetwork]
InputLayer: 500
HiddenLayer: 2000
OutputLayer: 500
EncoderActivation: ReLU
DecoderActivation: ReLU
LearningRate: 0.1
LREpochDecay: 73
MaxEpochs: 183
BatchSize: 256
CorruptionFactor: 0.2

[Experiment]
Num: 2
PREFIX: MNIST
InputFile: MNIST_hidden.npy
