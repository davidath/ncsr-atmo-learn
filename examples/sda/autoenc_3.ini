[NeuralNetwork]
InputLayer: 2000
HiddenLayer: 10
OutputLayer: 2000
EncoderActivation: Linear
DecoderActivation: ReLU
LearningRate: 0.1
LREpochDecay: 73
MaxEpochs: 183
BatchSize: 256
CorruptionFactor: 0.2

[Experiment]
Num: 3
PREFIX: MNIST
InputFile: MNIST_hidden.npy
